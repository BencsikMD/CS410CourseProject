{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow Text Classification using RNN\n",
    "\n",
    "This code started off as the tutorial from Tensor Flow. [TF RNN](https://www.tensorflow.org/text/tutorials/text_classification_rnn)\n",
    "\n",
    "I have made modifications to allow my datasets, ability to switch between datasets and optimizers,\n",
    "created my own code for splitting the datasets into training, validation and testing, batched the datasets,\n",
    "changed the NN models, added features for regularization of NN layers, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If controlTest_flag set to True, the model will run a control dataset, capable of high accuracy.\n",
    "#NOTE: this will download a dataset from Tensor Flow\n",
    "controlTest_flag = False\n",
    "# Setting retrainControlTest_flag to False will use the pre-trained model\n",
    "retrainControlTest_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Dataset\n",
    "\n",
    "1. Set `tickerSymbol` to a stock ticker from the list below. \n",
    "2. Set `textChoice` to 'title' or 'content'. Chooses whether to use the just the 'title' or entire 'content' from the news article. \n",
    "\n",
    "The datasets were created by filtering out a selected stock from this news archive dataset: \n",
    "[Kaggle - US Equities News Data](https://www.kaggle.com/datasets/gennadiyr/us-equities-news-data?resource=download)\n",
    "\n",
    "Then the historical price dataset for the stock was downloaded from: \n",
    "[Nasdaq - Historical Data](https://www.nasdaq.com/market-activity/quotes/historical)\n",
    "\n",
    "The 2 datasets were merged and cleaned up to contain only what was needed. The datasets contain the \n",
    "date (which was used for merging), the news article title or content, and a label indicating that the daily\n",
    "price increased (1) (or statyed the same) or decreased (0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a stock\n",
    "# 'AAPL' 'MSFT' 'AMZN' 'TSLA' 'NFLX' 'GOOGL' 'BA'  \n",
    "# 'Reddit' can also be used for the Reddit dataset. Note: only the title option can be used. \n",
    "tickerSymbol = 'AAPL'\n",
    "textChoice = 'title' # 'title' 'content'\n",
    "\n",
    "dataFile = '../Data/' + tickerSymbol + '_' + textChoice + '_' + 'NewsDataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the `SHUFFLE_SEED` to re-shuffle the dataset prior to running. \n",
    "\n",
    "The seed allows reproducibility to repeat a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this seed for a different shuffle. \n",
    "# Seed is here to create reproducible results, if needed.\n",
    "if not controlTest_flag:\n",
    "    SHUFFLE_SEED = 12345\n",
    "\n",
    "    stockDF = pd.read_csv(dataFile)\n",
    "    stockDF = stockDF.sample(frac=1, random_state=SHUFFLE_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BATCH_SIZE`, `TRAIN_PERCENT`, and `VALID_PERCENT` can be adjusted here.\n",
    "\n",
    "1. `BATCH_SIZE` is the number of text features per batch\n",
    "2. `TRAIN_PERCENT` is the percent of the dataset that is used for training vs. testing.\n",
    "3. `VALID_PERCENT` is the percent of the training dataset that is used for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not controlTest_flag:\n",
    "    N = len(stockDF)\n",
    "    BATCH_SIZE = 32\n",
    "    TRAIN_PERCENT = 0.8\n",
    "    VALID_PERCENT = 0.2\n",
    "\n",
    "    trainSize = int(N * TRAIN_PERCENT)\n",
    "    validSize = int(trainSize * VALID_PERCENT)\n",
    "\n",
    "    train_df = stockDF.iloc[:trainSize-validSize]\n",
    "    valid_df = stockDF.iloc[trainSize-validSize:trainSize]\n",
    "    test_df  = stockDF.iloc[trainSize: N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not controlTest_flag:\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_df['Text'],train_df['Label']))\n",
    "    valid_ds = tf.data.Dataset.from_tensor_slices((valid_df['Text'],valid_df['Label']))\n",
    "    test_ds  = tf.data.Dataset.from_tensor_slices((test_df['Text'],test_df['Label']))\n",
    "\n",
    "    train_ds = train_ds.batch(BATCH_SIZE)\n",
    "    valid_ds = valid_ds.batch(BATCH_SIZE)\n",
    "    test_ds  = test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if controlTest_flag:\n",
    "    import tensorflow_datasets as tfds\n",
    "    train_ds = tfds.load('imdb_reviews', split='train[:90%]', as_supervised=True)\n",
    "    valid_ds = tfds.load('imdb_reviews', split='train[90%:]', as_supervised=True)\n",
    "    test_ds = tfds.load('imdb_reviews', split= 'test', as_supervised=True)\n",
    "    #train_ds, test_ds = dataset['train'], dataset['test']\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "    train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    valid_ds = valid_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is performed using Keras built in tokenizer. In the cell below, parameters can be adjusted to affect tokenization.\n",
    "\n",
    "1. `VOCAB_SIZE` is the max vocab size created from the dataset\n",
    "2. `NGRAMS` allows ngrams to be used for tokenization. If a tuple is used, multiple ngrams are used. \n",
    "3. `TOKENIZATION_TYPE` allows for a standard index or batched index to be used. \n",
    "\n",
    "See [TF API Keras Vectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE          = 1000\n",
    "NGRAMS              = None      # None, 1, 2, etc. (1,2,3)\n",
    "TOKENIZATION_TYPE   = 'int'     # 'int', 'multi_hot', 'count', 'tf_idf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, ngrams=NGRAMS, output_mode=TOKENIZATION_TYPE)\n",
    "encoder.adapt(train_ds.map(lambda Text, Label: Text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 20 words of vocab created from data\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model\n",
    "\n",
    "Below is the model definition. \n",
    "\n",
    "1. `L2_REGULARIZATION` can be adjusted to reduce the weights of the model to make a more generic model and avoid over fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_REGULARIZATION = 0.000001    # Default = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(L2_REGULARIZATION))),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, kernel_regularizer=tf.keras.regularizers.l2(L2_REGULARIZATION))),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(L2_REGULARIZATION)),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "1. `EPOCHS` is the iteration of entire dataset ran during training.\n",
    "2. `VALIDATION_STEPS` Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n",
    "    1. See [TF Keras Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit)\n",
    "3. `LOSS_CLASS` sets the type of loss function used\n",
    "3. `OPTIMIZER` selects which optimizer from keras to use. \n",
    "    1. See [TF Keras Optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "4. `LEARNING_RATE` sets the learning rate of the optimizer. 0.03, 0.01, 0.003, 0.001, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS              = 5\n",
    "VALIDATION_STEPS    = None              # None 20, 40, etc. None will use all validation data\n",
    "LOSS_CLASS          = 'CrossEntropy'    # 'CrossEntropy' 'FocalCrossEntropy' \n",
    "OPTIMIZER           = 'Adam'            # 'Adam' 'Adadelta' 'SGD'\n",
    "LEARNING_RATE       = 1e-4              # Learning rate for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = {\n",
    "    'CrossEntropy'      : tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    'FocalCrossEntropy' : tf.keras.losses.BinaryFocalCrossentropy(from_logits=True)\n",
    "}\n",
    "\n",
    "optimizer = {\n",
    "    'Adam'      : tf.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    'Adadelta'  : tf.optimizers.Adadelta(),\n",
    "    'SGD'       : tf.optimizers.SGD()\n",
    "}\n",
    "\n",
    "metrics =   [   \n",
    "                tf.keras.metrics.BinaryCrossentropy(from_logits=True, name='binary_crossentropy'), \n",
    "                tf.keras.metrics.BinaryAccuracy(name='binary_accuracy'),\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=loss[LOSS_CLASS],\n",
    "              optimizer=optimizer[OPTIMIZER],\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "if not controlTest_flag or retrainControlTest_flag:\n",
    "    history = model.fit(train_ds, epochs=EPOCHS,\n",
    "                        validation_data=test_ds,\n",
    "                        validation_steps=VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('../Data/IMDB_RNNControlTestModel', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Testing the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if retrainControlTest_flag:\n",
    "    test_loss, test_bin, test_acc = model.evaluate(test_ds)\n",
    "else:\n",
    "    reloaded_model = tf.keras.models.load_model('../Data/IMDB_RNNControlTestModel')\n",
    "    reloaded_model.compile(loss=loss[LOSS_CLASS],\n",
    "              optimizer=optimizer[OPTIMIZER],\n",
    "              metrics=metrics)\n",
    "    test_loss, test_bin, test_acc = reloaded_model.evaluate(test_ds)\n",
    "\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Binary Cross Entropy:', test_bin)\n",
    "print('Test Accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_graphs(history, 'binary_accuracy')\n",
    "plt.subplot(2, 2, 2)\n",
    "plot_graphs(history, 'loss')\n",
    "plt.subplot(2, 2, 3)\n",
    "plot_graphs(history, 'binary_crossentropy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51d32a615755070830a1e212317cdba8790c54f51db92b1c7f00b70a02f13d97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
