{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow_text as tf_text\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = '../Data/Reddit_News_DJIA.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV data with tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = tf.data.TextLineDataset(fileName)\n",
    "rawData.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CSV data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditNewsDf = pd.read_csv(fileName)\n",
    "redditNewsDf.head(1)\n",
    "#redditNewsDf.Top1.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = redditNewsDf.shape\n",
    "cols = cols - 2 # subtract date and label\n",
    "print('Rows =', rows)\n",
    "print('Columns =', cols)\n",
    "N = rows * cols # N = Number of documents\n",
    "print('N: number of docs = ', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLabels = redditNewsDf.Label.values.reshape(-1,1)\n",
    "dfLabels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dfFeatures = np.char.encode(redditNewsDf.iloc[:,2:].values, encoding='utf-8')\n",
    "dfFeatures = redditNewsDf.iloc[:,2:].values\n",
    "dfFeatures = np.char.strip(np.asarray(dfFeatures,dtype=str),chars='b\\'\\\"')\n",
    "\n",
    "print(dfFeatures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeatures = dfFeatures.reshape(-1)\n",
    "dfFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLabels = dfLabels * np.ones((rows,cols))\n",
    "dfLabels = dfLabels.reshape(-1)\n",
    "dfLabels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines = [dfLabels,dfFeatures]\n",
    "\n",
    "featuresDS = tf.data.Dataset.from_tensor_slices(list(dfFeatures))\n",
    "labelDS =  tf.data.Dataset.from_tensor_slices(tf.cast(list(dfLabels), tf.int64))\n",
    "#linesDS = featuresDS.concatenate(tf.data.Dataset.from_tensor_slices(list(tf.cast(dfLabels,tf.int64))))\n",
    "#for i,row in featuresDS.enumerate().as_numpy_iterator():\n",
    "#    featuresDS.map(lambda element: [element, tf.cast(dfLabels[i], tf.int64)] )\n",
    "labeledDS = tf.data.Dataset.zip((featuresDS, labelDS))\n",
    "#list(labeledDS.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SIZE = 5000\n",
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "  lower_case = tf_text.case_fold_utf8(text)\n",
    "  return tokenizer.tokenize(lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = featuresDS.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch in tokenized_ds.take(5):\n",
    "  print(\"Tokens: \", text_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = configure_dataset(tokenized_ds)\n",
    "\n",
    "vocab_dict = collections.defaultdict(lambda: 0)\n",
    "for toks in tokenized_ds.as_numpy_iterator():\n",
    "  for tok in toks:\n",
    "    vocab_dict[tok] += 1\n",
    "\n",
    "vocab = sorted(vocab_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab = [token for token, count in vocab]\n",
    "vocab = vocab[:VOCAB_SIZE]\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "print(\"First five vocab entries:\", vocab[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPercent = 0.8\n",
    "dfFeatures_train = dfFeatures[:int(trainPercent*N)]\n",
    "dfLabels_train = dfLabels[:int(trainPercent*N)]\n",
    "dfFeatures_test = dfFeatures[int(trainPercent*N):]\n",
    "dfLabels_test = dfLabels[int(trainPercent*N):]\n",
    "\n",
    "print('Training features =',dfFeatures_train.shape)\n",
    "print('Training labels =',dfLabels_train.shape)\n",
    "print('Testing features =',dfFeatures_test.shape)\n",
    "print('Testing labels =',dfLabels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.asarray(dfFeatures,dtype=str))\n",
    "dfFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "#vectorizer.fit(np.asarray(dfFeatures_train,dtype=str))\n",
    "vectorizer.fit(dfFeatures_train)\n",
    "vocabulary = np.array(list(vectorizer.vocabulary_.items()))\n",
    "print('First 5 indexed vocab words:\\n', vocabulary[:5])\n",
    "M = vocabulary.shape[0]\n",
    "print('\\nNumber of words =', M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docMatrix_train = vectorizer.transform(dfFeatures_train).toarray()\n",
    "print(dfFeatures_train[0])\n",
    "print('First 5 rows of Doc Matrix:\\n', docMatrix_train[:5])\n",
    "\n",
    "print('\\nDoc Matrix Shape =', docMatrix_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docMatric_test = vectorizer.transform(dfFeatures_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(tol=0.1, max_iter=500)\n",
    "classifier.fit(docMatrix_train, dfLabels_train)\n",
    "score = classifier.score(docMatric_test, dfLabels_test)\n",
    "\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51d32a615755070830a1e212317cdba8790c54f51db92b1c7f00b70a02f13d97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
